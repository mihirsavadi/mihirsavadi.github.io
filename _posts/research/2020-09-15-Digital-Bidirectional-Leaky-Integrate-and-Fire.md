---
layout: single
classes: wide
title:  "[snn-#0] A Digital Bidirectional Leaky Integrate and Fire (DBLIF) Neural Model"
last_modified_at: 2020-12-16
categories: research snn

author: Mihir Savadi
---

<div markdown="1" style="font-size:20px; font-family: Times New Roman">
This is a short discussion of a simple digital hardware implementation similar to the leaky integrate and 
fire (LIF) model. The idea, in its most basic form, is illustrated in figure 1 below. Architectural decisions, 
their alternatives, and their implications are discussed further on.

<div><center><img src="/assets/research/snn0-fig1.png"></center></div>
*Figure 1: Functional Block Diagram of proposed DBLIF primitive*
{: .text-center}

The signal lifecycle through this model begins with its inputs, which are spiking outputs from previous neurons in the Spiking Neural Network (SNN). These act as ‘on/off switches’ to the signed weights associated with each of these inputs. These weights are then concurrently added through a combinational adder, the outputs of which are fed into an accumulator. The accumulator behaves like a ‘container’ that grows or leaks by $$[L + Adder Output]$$ increments every clock cycle. The output of this accumulator is then fed as a pointer to a look up table (LUT) containing any arbitrary activation function, thus introducing non-linearity to this otherwise so-far linear system. The output of the LUT $$f$$ is then compared to $$T$$ which is the neuron’s threshold parameter, via a combinational comparator circuit. If $$f > T$$ (i.e. not $$f ≤ T$$) then the neuron will produce a ‘1’ at $$Y$$, which is the neuron primitive’s output, from a prior 0 output state. The positive edge of this low to high transition on $$Y$$ would then trigger a reset flag in the aforementioned accumulator, which upon receiving its next clock edge would reset itself to 0 (or any other programmable value), zero the reset flag, and assuming that $$f(p_{LUT} = 0) = 0$$ or $$≤ T$$, cause $$Y$$ to drop back to 0. This would result in $$Y$$ producing pulses of width = $$clk$$ width, assuming that $$[x_1, x_2, ...,x_3]$$ are spikes produced by equivalent neural primitives. Also, because accumulator reset depends on a positive clock edge, starting states of the accumulator must be such that $$f ≤ T$$ so that $$Y = 0$$. The inputs layers of an SNN containing this model would require additional hardware in order to convert analog or ADC values to spikes, where rate coding seems to be a simple, attractive, and very neuromorphic solution. The output layer would then be interpreted via ‘reverse’ rate coding as well. <br/>

<!-- The behavior of the accumulator in conjunction with the presence of the known master clock $$clk$$ is what enables temporal encoding in the neural primitive. The primary parameter which can then be adjusted to influence the emphasis of the passage of time for a particular neuron is the leak rate $$L$$ parameter, but the weights obviously also contribute heavily. It is also important for the neural primitive to place variable emphasis on the time differences between any combination of spike inputs. This is done by being able to flexibly vary the relationship between the threshold potential $$T$$, leak rate $$L$$, and weights in each neuron. With backpropagation performed with $$T$$ and $$L$$ as additional parameters to the weight vector in a multi-layer NN layout of sufficient layers, implicit memory may arise about the significance between any combination of specific localized spiking events. The bidirectionality (i.e. signed nature) of the $$L$$ parameter may help insert greater flexibility of the neurons to achieve this generation of implicit memory with fewer layers. This hypothesis was reached not through rigorous math proof, rather extrapolation of intuition, so I might be completely wrong here from a mathematical perspective alone. <br/> -->

The presence of a known master clock is potentially useful, especially considering real-time learning applications in SoC (system on chip) hardware implementations. Because the SNN extracts meaning from sequentially meaningful events, training data cannot be static. Take multiple short sound samples as an example of a potential set of training data. If we know the true real time rate of playback, we can easily scale up the playback rate and equivalently scale up the clock rate of the SNN so that it can parse through learning data at a rate much faster than the true passage of time, which would help expedite calculation of error-functions and by extension the backpropagation process as a whole, while maintaining accurate real-time replicability of the resultant optimised parameters (i.e. not losing temporal context). Besides the existing problems surrounding backpropagation and SNN's, Gradient derivation and descent calculations would however still be a bottleneck in terms of total backpropagation turnaround times.<br/>

Another interesting neuromorphic characteristic this model provides is the emulation of the ‘refractory period’ in biological neurons, where-by there is a state of inactivity of the neuron for a certain amount of time after a firing event. This is emulated by the accumulator resetting on an output pulse, and can be further accentuated by lowering values for combinations of $$L$$ or $$T$$ or weight parameters. <br/>

One drawback that immediately comes to mind with this model is the digital-resource costliness per neuron with the potentially expansive adder circuit and LUT present in each neural primitive. This can be worked around by having a single adder circuit and look up table for all neurons in the network, which would then need to be sequentially accessed via a network-wide scheduling system. A secondary clock much faster than $$clk$$ would have to be implemented such that all neurons in the network can calculate and update various individual parameters using the ‘communal’ adder and look up table within a single time period of the main clock (or equivalently passing $$clk$$ a scaled down master clock signal). This could potentially mean significantly scaling down $$clk$$ rates such that each neuron can be updated in time. However, FPGA’s nowadays are easily able to clock in the hundreds of MHz range, and biological neural systems arent known to be able to parse real time sensory input information at update rates of more than a few hundred Hz, so this *might* not become a costly trade-off as quickly as one might presume. Another thing to consider however would be the point of NN complexity at which the aforementioned network-wide scheduler resources would rival the resources needed if each neuron stored its own adder and LUT – how each method scales would be useful to look into. <br/>

The LUT may even be avoided entirely by simply implementing a time-constant multiplier after the accumulator, i.e. $$g(x) = e^x$$ whereby $$x$$ is the output of the multiplier and $$g$$ is the function replacing the LUT. Issues once again arise around the complex hardware needed for each Neuron in order to implement this function. Additionally, foregoing the LUT for this method would remove the ability to insert arbitrary activation functions - the significance of which is left to be determined. From intuition, LUT's of reasonable granularity and word sizes (e.g. 16 or 32 bit address spaces with 16 or 32 bit word sizes) would be massive, making the time-constant multiplier seem like a more efficient alternative in terms of FPGA synthesis. Either way, the nuances of the tradeoffs between the number of nuerons we can fit in a single network and the complexity of each nueron would be interesting to study. <br/>

From trial and error synthesizing for an Altera Cyclone V SX SoC—5CSXFC6D6F31C6N (a lower end consumer level FPGA) & Quartus, a single dblif model from figure 1 with 10x16bit weights, a 32bit accumulator, and no LUT, produced a quartus reported utilization rate in ALM's of 58/41910, allowing for a capacity of around 722 dblif nuerons best case with room for no other hardware - an unlikely situation. A more reasonable estimate would be around the high 600's depending on the amount of other hardware necessary to support the network.
</div>

